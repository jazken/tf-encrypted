{
 "cells": [
  {
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tf_encrypted as tfe\n",
    "\n",
    "#from convert import decode "
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 5
  },
  {
   "source": [
    "## data processing helpers\n",
    "\n",
    "def encode_image(value):\n",
    "  \"\"\"Encode images into a tf.train.Feature for a TFRecord.\"\"\"\n",
    "  bytes_list = tf.train.BytesList(value=[value.tostring()])\n",
    "  return tf.train.Feature(bytes_list=bytes_list)\n",
    "\n",
    "\n",
    "def decode_image(value):\n",
    "  \"\"\"Decode the image from a tf.train.Feature in a TFRecord.\"\"\"\n",
    "  image = tf.decode_raw(value, tf.uint8)\n",
    "  image.set_shape((28 * 28))\n",
    "  return image\n",
    "\n",
    "\n",
    "def encode_label(value):\n",
    "  \"\"\"Encode a label into a tf.train.Feature for a TFRecord.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def decode_label(value):\n",
    "  \"\"\"Decode the label from a tf.train.Feature in a TFRecord.\"\"\"\n",
    "  return tf.cast(value, tf.int32)\n",
    "\n",
    "\n",
    "def encode(image, label):\n",
    "  \"\"\"Encode an instance as a tf.train.Example for a TFRecord.\"\"\"\n",
    "  feature_dict = {'image': encode_image(image), 'label': encode_label(label)}\n",
    "  features = tf.train.Features(feature=feature_dict)\n",
    "  return tf.train.Example(features=features)\n",
    "\n",
    "\n",
    "def decode(serialized_example):\n",
    "  \"\"\"Decode an instance from a tf.train.Example in a TFRecord.\"\"\"\n",
    "  features = tf.parse_single_example(serialized_example, features={\n",
    "      'image': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64)\n",
    "  })\n",
    "  image = decode_image(features['image'])\n",
    "  label = decode_label(features['label'])\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def normalize(image, label):\n",
    "  \"\"\"Standardization of MNIST images.\"\"\"\n",
    "  x = tf.cast(image, tf.float32) / 255.\n",
    "  image = (x - 0.1307) / 0.3081  # image = (x - mean) / std\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def get_data_from_tfrecord(filename, batch_size: int):\n",
    "  \"\"\"Construct a TFRecordDataset iterator.\"\"\"\n",
    "  return tf.data.TFRecordDataset([filename]) \\\n",
    "                .map(decode) \\\n",
    "                .map(normalize) \\\n",
    "                .repeat() \\\n",
    "                .batch(batch_size) \\\n",
    "                .make_one_shot_iterator()"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-58652160812e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;31m# config file was specified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mconfig_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m   \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemoteConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m   \u001b[0mtfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[0mtfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_protocol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\config.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mstr\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mName\u001b[0m \u001b[0mof\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[1;32mfrom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \"\"\"\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m       \u001b[0mhostmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mRemoteConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhostmap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "## only used when it's running on server\n",
    "## have to look at how to set config file\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "  # config file was specified\n",
    "  config_file = sys.argv[1]\n",
    "  config = tfe.RemoteConfig.load(config_file)\n",
    "  tfe.set_config(config)\n",
    "  tfe.set_protocol(tfe.protocol.Pond())\n",
    "\n",
    "session_target = sys.argv[2] if len(sys.argv) > 2 else None"
   ]
  },
  {
   "source": [
    "## Model Owner Class\n",
    "class ModelOwner():\n",
    "  \"\"\"Contains code meant to be executed by the model owner.\n",
    "\n",
    "  Args:\n",
    "    player_name: `str`, name of the `tfe.player.Player`\n",
    "                 representing the model owner.\n",
    "    local_data_file: filepath to MNIST data.\n",
    "  \"\"\"\n",
    "  BATCH_SIZE = 128\n",
    "  NUM_CLASSES = 10\n",
    "  EPOCHS = 1\n",
    "\n",
    "  ITERATIONS = 60000 // BATCH_SIZE\n",
    "\n",
    "  IMG_ROWS = 28\n",
    "  IMG_COLS = 28\n",
    "  FLATTENED_DIM = IMG_ROWS * IMG_COLS\n",
    "\n",
    "  def __init__(self, player_name, local_data_file):\n",
    "    self.player_name = player_name\n",
    "    self.local_data_file = local_data_file\n",
    "\n",
    "  def _build_data_pipeline(self):\n",
    "    \"\"\"Build a reproducible tf.data iterator.\"\"\"\n",
    "\n",
    "    def normalize(image, label):\n",
    "      image = tf.cast(image, tf.float32) / 255.0\n",
    "      return image, label\n",
    "\n",
    "    def flatten(image, label):\n",
    "      image = tf.reshape(image, shape=[self.FLATTENED_DIM])\n",
    "      return image, label\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset([self.local_data_file])\n",
    "    dataset = dataset.map(decode)\n",
    "    dataset = dataset.map(normalize)\n",
    "    dataset = dataset.map(flatten)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(self.BATCH_SIZE)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator\n",
    "\n",
    "  def _build_training_graph(self, training_data):\n",
    "    \"\"\"Build a graph for plaintext model training.\"\"\"\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(512, input_shape=[self.FLATTENED_DIM,]))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dense(self.NUM_CLASSES, activation=None))\n",
    "\n",
    "    # optimizer and data pipeline\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "    def loss(model, inputs, targets):\n",
    "      logits = model(inputs)\n",
    "      per_element_loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "          labels=targets, logits=logits)\n",
    "      return tf.reduce_mean(per_element_loss)\n",
    "\n",
    "    def grad(model, inputs, targets):\n",
    "      loss_value = loss(model, inputs, targets)\n",
    "      return loss_value, tf.gradients(loss_value, model.trainable_variables)\n",
    "\n",
    "    def loop_body(i):\n",
    "      x, y = training_data.get_next()\n",
    "      _, grads = grad(model, x, y)\n",
    "      update_op = optimizer.apply_gradients(\n",
    "          zip(grads, model.trainable_variables))\n",
    "      with tf.control_dependencies([update_op]):\n",
    "        return i + 1\n",
    "\n",
    "    loop = tf.while_loop(lambda i: i < self.ITERATIONS * self.EPOCHS,\n",
    "                         loop_body, loop_vars=(0,))\n",
    "\n",
    "    with tf.control_dependencies([loop]):\n",
    "      print_op = tf.print(\"Training complete\")\n",
    "    with tf.control_dependencies([print_op]):\n",
    "      return [tf.identity(x) for x in model.trainable_variables]\n",
    "\n",
    "  @tfe.local_computation\n",
    "  def provide_weights(self):\n",
    "    with tf.name_scope('loading'):\n",
    "      training_data = self._build_data_pipeline()\n",
    "\n",
    "    with tf.name_scope('training'):\n",
    "      parameters = self._build_training_graph(training_data)\n",
    "\n",
    "    return parameters"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 7
  },
  {
   "source": [
    "## Prediction Client\n",
    "class PredictionClient():\n",
    "  \"\"\"\n",
    "  Contains code meant to be executed by a prediction client.\n",
    "\n",
    "  Args:\n",
    "    player_name: `str`, name of the `tfe.player.Player`\n",
    "                 representing the data owner\n",
    "    build_update_step: `Callable`, the function used to construct\n",
    "                       a local federated learning update.\n",
    "  \"\"\"\n",
    "\n",
    "  BATCH_SIZE = 20\n",
    "\n",
    "  def __init__(self, player_name, local_data_file):\n",
    "    self.player_name = player_name\n",
    "    self.local_data_file = local_data_file\n",
    "\n",
    "  def _build_data_pipeline(self):\n",
    "    \"\"\"Build a reproducible tf.data iterator.\"\"\"\n",
    "\n",
    "    def normalize(image, label):\n",
    "      image = tf.cast(image, tf.float32) / 255.0\n",
    "      return image, label\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset([self.local_data_file])\n",
    "    dataset = dataset.map(decode)\n",
    "    dataset = dataset.map(normalize)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(self.BATCH_SIZE)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator\n",
    "\n",
    "  @tfe.local_computation\n",
    "  def provide_input(self) -> tf.Tensor:\n",
    "    \"\"\"Prepare input data for prediction.\"\"\"\n",
    "    with tf.name_scope('loading'):\n",
    "      prediction_input, expected_result = self._build_data_pipeline().get_next()\n",
    "      print_op = tf.print(\"Expect\", expected_result, summarize=self.BATCH_SIZE)\n",
    "      with tf.control_dependencies([print_op]):\n",
    "        prediction_input = tf.identity(prediction_input)\n",
    "\n",
    "    with tf.name_scope('pre-processing'):\n",
    "      prediction_input = tf.reshape(\n",
    "          prediction_input, shape=(self.BATCH_SIZE, ModelOwner.FLATTENED_DIM))\n",
    "    return prediction_input\n",
    "\n",
    "  @tfe.local_computation\n",
    "  def receive_output(self, logits: tf.Tensor) -> tf.Operation:\n",
    "    with tf.name_scope('post-processing'):\n",
    "      prediction = tf.argmax(logits, axis=1)\n",
    "      op = tf.print(\"Result\", prediction, summarize=self.BATCH_SIZE)\n",
    "      return op"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 4
  },
  {
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "model_owner = ModelOwner(\n",
    "  player_name=\"model-owner\",\n",
    "  local_data_file=\"./data/train.tfrecord\")\n",
    "\n",
    "prediction_client = PredictionClient(\n",
    "  player_name=\"prediction-client\",\n",
    "  local_data_file=\"./data/test.tfrecord\")\n",
    "\n",
    "# get model parameters as private tensors from model owner\n",
    "params = model_owner.provide_weights()\n",
    "\n",
    "# we'll use the same parameters for each prediction so we cache them to\n",
    "# avoid re-training each time\n",
    "cache_updater, params = tfe.cache(params)\n",
    "\n",
    "with tfe.protocol.SecureNN():\n",
    "# get prediction input from client\n",
    "  x = prediction_client.provide_input()\n",
    "\n",
    "  model = tfe.keras.Sequential()\n",
    "  model.add(tfe.keras.layers.Dense(512, batch_input_shape=x.shape))\n",
    "  model.add(tfe.keras.layers.Activation('relu'))\n",
    "  model.add(tfe.keras.layers.Dense(10, activation=None))\n",
    "\n",
    "  logits = model(x)\n",
    "\n",
    "  # send prediction output back to client\n",
    "  prediction_op = prediction_client.receive_output(logits)\n",
    "\n",
    "with tfe.Session(target=session_target) as sess:\n",
    "  sess.run(tf.global_variables_initializer(), tag='init')\n",
    "\n",
    "  print(\"Training\")\n",
    "  sess.run(cache_updater, tag='training')\n",
    "\n",
    "  print(\"Set trained weights\")\n",
    "  model.set_weights(params, sess)\n",
    "\n",
    "  for _ in range(5):\n",
    "    print(\"Predicting\")\n",
    "    sess.run(prediction_op, tag='prediction')"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\autograph\\converters\\directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\autograph\\converters\\directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\autograph\\converters\\directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\autograph\\converters\\directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n\nWARNING:tensorflow:From <ipython-input-7-7014a213ea2f>:42: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\nWARNING:tensorflow:From <ipython-input-7-7014a213ea2f>:42: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\ops\\losses\\losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tensorflow_core\\python\\ops\\losses\\losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\tensor\\native.py:101: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\tensor\\native.py:101: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\tensor\\native.py:445: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\tensor\\native.py:445: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\keras\\engine\\base_layer_utils.py:29: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\keras\\engine\\base_layer_utils.py:29: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\tensor\\native.py:415: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted\\tensor\\native.py:415: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'session_target' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3215bda35e6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m   \u001b[0mprediction_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession_target\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m   \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'init'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'session_target' is not defined"
     ]
    }
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}